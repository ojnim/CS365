Large language models create contextual embeddings of the words in their input, starting with a
static embedding of each token and progressively
adding more contextual information in each layer
(Devlin et al., 2019; Brown et al., 2020; Manning et al., 2020). While these contextual embedding models are often praised for capturing
rich grammatical structure, a spate of recent work
has shown that they are surprisingly invariant to
scrambling word order (Sinha et al., 2021; Hessel and Schofield, 2021; Pham et al., 2021; Gupta
et al., 2021; O’Connor and Andreas, 2021) and
that grammatical knowledge like part of speech,
often attributed to contextual embeddings, is actually also captured by fixed embeddings (Pimentel
et al., 2020). These results point to a puzzle: how
can syntactic contextual information be important
for language understanding when the words themselves, not their order, are what matter?
We argue that this apparent paradox arises because of the redundant structure of language itself. Lexical distributional information alone inherently captures a great deal of meaning (Erk, 2012;
Mitchell and Lapata, 2010; Tal and Arnon, 2022),
and typically both humans and machines can reconstruct meanings of sentences under local scrambling of words (Mollica et al., 2020; Clouatre et al.,
2021). In this paper, we study model behaviour in
cases where word order is informative and is not
redundant with lexical information.
We focus on the feature of grammatical role
(whether a noun is the subject or the object of a
clause). Most natural clauses are prototypical:
in a sentence like “the chef chopped the onion”,
the grammatical roles of chef and onion are clear
to humans from the words alone, without word
order or context (see Mahowald et al., 2022, for
experiments in English and Russian in which human participants successfully guessed which of
two nouns was the subject and which was the object of a simple transitive clause, in the absence
of word order and contextual information). This
means syntactic word order is often redundant with
lexical semantics. Whether hand-constructed or
corpus-based, most studies probing contextual representations have used prototypical sentences as
input, where syntactic word order may not have
much information to contribute to core meaning
beyond the words themselves.
Yet human language can use syntax to deviate
from the expectations generated by lexical items:
we can also understand the absurd meaning of a
rare non-prototypical sentence like “The onion
chopped the chef” (Garrett, 1976; Gibson et al.,
2013). Is this use of syntactic word order available
to pretrained models? In this paper, we train grammatical role probes on the embedding spaces of
BERT and GPT-21
, and evaluate them on these rare
non-prototypical examples, where the meaning of
words in context is different from what we would
expect from looking at the words alone. We focus
on English because grammatical role is directly dependent on word order in English, and because we
had access to sufficiently large English parsed corpora such that we could generate non-prototypical
sentences, easily check them, and filter to grammatical ones.
We probe for grammatical role because it is key
to the basic compositional semantic structure of a
sentence (Dixon, 1979; Comrie, 1989; Croft, 2001).
While fixed lexical semantics contains information
about grammatical role (animate nouns are likely to
be subjects, etc), the grammatical role of a word in
English is ultimately determined by syntactic word
order. Probing grammatical role lets us examine
the interplay between syntactic word order and lexical semantics in forming compositional meaning
through model layers.
For all of our experiments, we train grammatical
role probes with standard data and test them on
either prototypical cases or non-prototypical cases
(where word order matters), to understand if grammatical embedding under normal circumstances is
sensitive to word order. Our experiments reveal
three key findings:
1. Lexical semantics plays a key role in organizing embedding space in early layer representations, and non-lexical compositional
features are expressed gradually in later layers, as shown by probe performance on nonprototypical sentences (Experiment 1, Figure
1).
2. Embeddings represent meaning that is imparted only by syntactic word order, overriding lexical and distributional cues. When we
control for distributional co-occurrence factors by evaluating our probes on argument
swapped sentences (like “The onion chopped
the chef”, real sample in Appendix B), probes
can differentiate the same word in different
roles (Experiment 2, Figure 2).
3. Syntactic word order is significant beyond just
local coherence: the compositional information of syntactic word order is lost when we
test our probes on locally-shuffled sentences,
that keep local lexical coherence but break
acute syntactic relations (Figure 3).
More generally, we highlight the importance of
examining models using non-prototypical examples, both for understanding the strength of lexical
influence in contextual embeddings, but also for
accurately isolating syntactic processing where it
is taking place.
While recent work has shown that large language
models come to rely largely on distributional semantic information, we consider the model’s ability to overcome these distributional cues. Research
showing that models rely on lexical and distributional information is not at odds with our findings
that this can be overridden. In fact, even though humans can accurately understand non-prototypical
sentences, human syntactic processing is often influenced by the lexical semantics of words, as evidenced by studies on human subjects (Frazier and
Rayner, 1982; Rayner et al., 1983; Ferreira and
Henderson, 1990) as well as by lexically-influenced
syntactic processes in human languages, like differential object marking (Aissen, 2003)—a phenomenon whereby non-prototypical grammatical
objects are marked.
More generally, while we have shown that it is
tempting for a straightforward probing approach
to conclude that grammatical role information is
available to the lowest layers of BERT, separately
analyzing prototypical and non-prototypical arguments makes it clear that the picture is more complicated. At lower layers, BERT representations can
typically classify subjects and objects, but when
a non-prototypical meaning is expressed, accurate
classification is not available until the higher layers.
We argue that considering probing performance
on these non-prototypical instances is crucial. A
key design feature of human language is the ability to talk about things that aren’t there or don’t
exist (Hockett, 1960), and it has been argued that
the combinatoric power of syntax exists to allow
humans to say things that are subtle, surprising, or
impossible (Garrett, 1976; Chomsky, 1957). Thus,
considering probing accuracy on the average task
may be misleading. Insofar as being able to understand non-prototypical meanings is a hallmark
of human language and insofar as these meanings
may differ in systematic ways from prototypical
meanings, considering such cases is crucial for
understanding how language models represent language.