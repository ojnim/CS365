End-to-end automatic speech translation (AST) relies on data that consist only of speech inputs and
corresponding translations. Such data are notoriously limited. Data augmentation approaches attempt to compensate the scarcity of such data by
generating synthetic data by translating transcripts
into foreign languages or by back-translating targetlanguage data via text-to-speech synthesis (TTS)
(Pino et al., 2019; Jia et al., 2019), or by performing
knowledge distillation using a translation system
trained on gold standard transcripts and reference
translations (Inaguma et al., 2021). In this paper,
we present a simple, resource conserving approach
that does not require TTS and yields improvements
complementary to knowledge distillation (KD).
For training cascaded systems, monolingual data
for automatic speech recognition and textual translation data for machine translation can be used, reducing the problem of scarcity. Cascaded systems,
however, suffer from error propagation, which has
been addressed by using more complex intermediate representations such as n-best machine translation (MT) outputs or lattices (Bertoldi and Federico,
2005; Beck et al., 2019, inter alia) or by modifying
training data to incorporate errors from automatic
speech recognition (ASR) and MT (Ruiz et al.,
2015; Lam et al., 2021b). End-to-end systems are
unaffected by this kind of error propagation and are
able to surpass cascaded systems if trained on sufficient amounts of data (Sperber and Paulik, 2020).
Our approach transfers an idea on aligned data
augmentation that has been presented for ASR
(Lam et al., 2021a) to aligned data augmentation
in AST. Similar to aligned data augmentation for
ASR, we utilize forced alignment information to
create unseen training pairs in a structured manner.
Our augmentation procedure consists of the following steps: (1) Sampling of a replacement suffix of a
transcription and its aligned speech representations,
guided by linguistic constraints. (2) Translation
of the transcription containing the new suffix. (3)
Recombination of audio data containing the new
suffix and the generated translation to distill a new
training pair. We thus use the acronym STR (Sample, Translate, Recombine) to refer to our method.
In comparison to Pino et al. (2019) and Jia et al.
(2019) who use TTS to generate synthetic speech,
we create new examples by recombining real human speech. This reduces the problem of overfitting to synthetic data as for example in SkinAugment (McCarthy et al., 2020) where synthetic audio
is generated by auto-encoding speaker conversions.
The basic idea of our method is comparable to data
augmentation techniques for images such as CutMix (Yun et al., 2019) where images are blended
together to form new data examples. However, CutMix selects images randomly, while we recombine
phrases in a structured manner.
Our experimental evaluation is conducted for
five language pairs on the CoVoST 2 dataset (Wang
et al., 2021) and for two language pairs on the
Europarl-ST (Iranzo-Sánchez et al., 2020) dataset.
We find considerable improvements for all language pairs on all datasets for our approach on
top of KD. Our approach can be seen as an enhancement of Inaguma et al. (2021)’s KD approach
since it requires roughly the same computational
resources and consistently improves their gains.
We proposed an effective data augmentation
method for end-to-end speech translation which
leverages audio alignments, linguistic properties,
and translation. It creates new audio-translation
pairs via sampling from a memory-efficient suffix
memory, translating through an MT model and recombining original and sampled audio segments
with translations. Our method achieves significant improvements over augmentation with KD
alone on both large (CoVoST 2) and small scale
(Europarl-ST) datasets. In future work, we would
like to investigate the utility of other linguistic properties for AST augmentation and we would like to
extend our method to multilingual AST.