The use of contextual embedding techniques is receiving more and more attention in the field of
semantic shift detection. In particular, pre-trained
models like BERT (Hu et al., 2019; Martinc et al.,
2020a), ELMo (Kutuzov and Giulianelli, 2020;
Rodina et al., 2020), and XLM-R (Cuba Gyllensten et al., 2020; Rother et al., 2020), are being
proposed as promising solutions to capture the
different meanings of a target word according to
the different contexts in which the word appears
throughout a considered diachronic corpus. Such
solutions generally employ clustering techniques
to aggregate embeddings of a specific word into
clusters (Martinc et al., 2020a; Karnysheva and
Schwarz, 2020). The idea is that each cluster denotes a specific word meaning that can be recognized in the considered documents. In this way,
it is possible to analyze the shift of a word meaning/sense by exploiting the evolution of a cluster
over time. For instance, an increasing number of
elements in a cluster denotes that the associated
word meaning is getting frequently adopted. On
the opposite, a cluster with a decreasing number of
elements over time refers to a word meaning that
is getting obsolete. Usually, the corpus is static,
meaning that all the documents of the considered
time periods are available as one whole, and a single clustering activity is performed over the entire corpus, generating clusters of word meaning
with documents of different time periods (Kutuzov
et al., 2018; Tahmasebi et al., 2018, 2021). As
a result, the time period in which a document is
added to the corpus is not taken into account for
cluster composition, and this is not completely satisfactory for an appropriate recognition of meaning
changes over time. When a dynamic corpus is
considered, namely time periods and documents
can be progressively added, scalability issues also
arise, since the clusters of word meanings need to
be re-calculated or updated. As a possible solution, some recent works propose to perform clustering separately for each time period. In this case,
the resulting clusters need to be aligned in order
to recognize similar word meanings in different,
consecutive time periods (Kanjirangat et al., 2020;
Montariol et al., 2021). However, solutions based
on clustering alignment are not satisfactory as well,
since they do not capture the possible evolution
pattern of a meaning across different time periods. A recent work proposes an average-based
approach to track semantic shift via continuously
evolving embeddings (Horn, 2021) computed as
a weighted running average (Finch, 2009) of embeddings generated by a contextual model. This
method is suitable to be applied on stream data and
it is far more scalable than typically cluster-based
methods. Nevertheless, it does not allow to analyse
which meanings are actually changed.
In this paper, we present What is Done is Done
(WiDiD), an incremental approach to semantic shift
detection based on incremental clustering techniques and contextual embeddings to capture the
changes over the meanings of a target word along a
diachronic corpus. In WiDiD, we work under the assumption that the documents of the corpus become
available as a stream and they are segmented in a
sequence of time periods. The word contexts observed in past time periods are consolidated as a set
of clusters that constitute the “memory” of the word
meanings observed so far. Such a memory is then
exploited as a basis for subsequent word observations in the current time period. The idea of WiDiD
is that the clusters of word meanings previously
created cannot be changed (what is done is done),
and the word meanings that are observed in the
present must be stratified/integrated over the past
ones. To enforce scalability, incremental clustering
techniques are employed in WiDiD, so that the word
embeddings extracted from the documents of the
current time period are compared and assimilated
into the set of consolidated clusters coming from
the past time periods. A comparative evaluation of
the proposed WiDiD approach against a reference
benchmark is discussed in the paper according to
multiple configurations characterized by different
clustering algorithms and embedding methods. In
particular, we present experiments based on a pretrained BERT model as well as results obtained
from a trained Doc2Vec model, which has been
adapted to provide pseudo-contextual word embeddings to extend the conventional static word representations of context-free embedding techniques.
As a further contribution of WiDiD, different metrics
for semantic shift evaluation of word meanings are
defined in the paper and experimental results are
provided to discuss their effectiveness.
The paper is organized as follows. In Section 2,
the relevant literature is discussed. In Section 3, we
present the WiDiD approach. Incremental clustering
techniques and semantic shift measures of WiDiD
are illustrated in Sections 4 and 5, respectively.
Experimental results are discussed in Section 6.
Section 7 finally provides our concluding remarks.
In this paper, we presented the WiDiD approach
characterized by incremental clustering techniques
and contextual word embedding methods. Ongoing work is about the fine-tuning of adopted embedding models to further improve the quality of
results. Moreover, we are working on defining cluster analysis techniques. The idea is to exploit the
results of semantic shift measures to interpret possible trend patterns over clusters along the time,
such as a broad meaning that forks into narrower
ones, or a meaning that increases its popularity and
vice versa. Further work is about the specification
of aging policies to manage the memory of aged
embeddings in the cluster evolution.