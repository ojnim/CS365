Readability is proved to be an objective and consis- tent (Fry, 2002) criterion to level reading materials for language learners. Leveled reading materials are extensively needed, since language learners at different stages of language acquisition need read- ings at different readability levels to build up their reading skills (Kasule, 2011; Alowais and Ogdol, 2021; Pitcher and Fang, 2007). However, judg- ing and selecting the readability levels of materials need time and professional knowledge, which is quite inefficient compared to the ever-increasing demand. To address the need for automatically assessing the readability level of a given text, Auto- matic Readability Assessment (ARA) is proposed.
In the early time, experts design formulas (Lennon and Burdick, 2004; Chall and Dale, 1995; Mc Laughlin, 1969; Flesch, 1948) based on the statistics from text such as word length and sen- tence length. Later, researchers (Feng et al., 2010; McCarthy and Jarvis, 2010; Kate et al., 2010; Vaj- jala and Meurers, 2012) mine useful morphologi- cal, lexical, syntactic and discourse features from text and use them with traditional machine learning models.
Deep learning models such as RNN-based mod- els (Azpiazu and Pera, 2019; Yang et al., 2016) automatically learn dense word embeddings re- lated to the readability of the texts. Recently, the popular pre-trained language models (PLMs) like BERT (Devlin et al., 2019) with their representative dense embeddings are also reported effective (Mar- tinc et al., 2021) on ARA . However, researchers also find handicaps of these deep learning models. Since organizing large-scaled ARA dataset is difficult due to the time and expertise required, datasets used in ARA are relatively small. The insufficiency of data makes it difficult to train a reliable deep learning model (Lee et al., 2021). What’s more, as the materials are designed to guide learners step by step, while describing the same thing, the word use, the structure of sentences and the manner of writing the full passages are made stratified inten- tionally, which is hard to detect for PLMs inclined to semantic information (Martinc et al., 2021; Qiu et al., 2021). For these reasons, some of them in- corporate linguistic features with PLMs (Lee et al., 2021; Qiu et al., 2021) and achieve improvements.
Despite their initial success, insufficient analysis of the long passage characteristic of ARA has been done before. We notice that the length of passages in ARA datasets consisting of reading materials can easily go beyond the capacity of PLMs (usually 510 tokens). Specifically, as shown in Fig 1, most ARA datasets have more than 50% passages longer than 510 tokens. Through preliminary experiments (Table 2 last row), we find that such a small dataset is not sufficient to train long-document 
transformers such as CogLTX (Ding et al., 2020) since they usually have more parameters. Besides, splitting passages into shorter pieces and directly congregating them will lose their inner relation, which is sub-optimal for ARA, since characteristics such as the number of theme and the intertextual depen- dence1 are important for deciding the readability level. From this point of view, linguistic features ex- tracted from the whole passage actually provide us information from a holistic view, and it can be eas- ily integrated into the models we are using. In this paper, we integrate linguistic features with PLMs and conduct abundant experiments to analyze the effect of linguistic features on ARA from the per- spective of passage length. We find that:
• Evenwithsimplelinguisticfeatures,theaccuracy of PLMs on those small-scaled datasets (OneStop and RAZ) greatly improves by 9% and 22% re- spectively. Error analysis shows that all of the improvements are on long passages more than 510 tokens.
• The promotion of the features on PLMs becomes less significant when the dataset size exceeds ∼ 750 passages.
• Our results suggest that Newsela is possibly not suitable for ARA.
Also, we construct an up-to-date and high-quality dataset called RAZ from RAZ-Kid2’s printed lev- eled books. Though small-scaled, texts from this popular website make our research more practical.
In this paper, we investigate how linguistic fea- tures promote PLMs on ARA from the perspective of passage length. Firstly, two self-proposed hy- potheses are proved: 1. Linguistic features provide linguistic information that PLM is not good at cap- turing; 2. Linguistic features provide information about the segment dropped by PLM. Secondly, we observe that the promotion of the features on PLMs becomes less significant when the dataset size ex- ceeds ∼750 passages. Thirdly, our results suggest that Newsela dataset is possibly not suitable for ARA.
