1 Introduction
Readability is proved to be an objective and consis- tent (Fry, 2002) criterion to level reading materials for language learners. Leveled reading materials are extensively needed, since language learners at different stages of language acquisition need read- ings at different readability levels to build up their reading skills (Kasule, 2011; Alowais and Ogdol, 2021; Pitcher and Fang, 2007). However, judg- ing and selecting the readability levels of materials need time and professional knowledge, which is quite inefficient compared to the ever-increasing demand. To address the need for automatically assessing the readability level of a given text, Auto- matic Readability Assessment (ARA) is proposed.
In the early time, experts design formulas (Lennon and Burdick, 2004; Chall and Dale, 1995;
Mc Laughlin, 1969; Flesch, 1948) based on the statistics from text such as word length and sen- tence length. Later, researchers (Feng et al., 2010; McCarthy and Jarvis, 2010; Kate et al., 2010; Vaj- jala and Meurers, 2012) mine useful morphologi- cal, lexical, syntactic and discourse features from text and use them with traditional machine learning models.
Deep learning models such as RNN-based mod- els (Azpiazu and Pera, 2019; Yang et al., 2016) automatically learn dense word embeddings re- lated to the readability of the texts. Recently, the popular pre-trained language models (PLMs) like BERT (Devlin et al., 2019) with their representative dense embeddings are also reported effective (Mar- tinc et al., 2021) on ARA . However, researchers also find handicaps of these deep learning models. Since organizing large-scaled ARA dataset is diffi- cult due to the time and expertise required, datasets used in ARA are relatively small. The insufficiency of data makes it difficult to train a reliable deep learning model (Lee et al., 2021). What’s more, as the materials are designed to guide learners step by step, while describing the same thing, the word use, the structure of sentences and the manner of writing the full passages are made stratified inten- tionally, which is hard to detect for PLMs inclined to semantic information (Martinc et al., 2021; Qiu et al., 2021). For these reasons, some of them in- corporate linguistic features with PLMs (Lee et al., 2021; Qiu et al., 2021) and achieve improvements.
Despite their initial success, insufficient analysis of the long passage characteristic of ARA has been done before. We notice that the length of passages in ARA datasets consisting of reading materials can easily go beyond the capacity of PLMs (usually 510 tokens). Specifically, as shown in Fig 1, most ARA datasets have more than 50% passages longer than 510 tokens. Through preliminary experiments (Table 2 last row), we find that such a small dataset is not sufficient to train long-document transformers such as CogLTX (Ding et al., 2020) since they usually have more parameters. Besides, splitting passages into shorter pieces and directly congre- gating them will lose their inner relation, which is sub-optimal for ARA, since characteristics such as the number of theme and the intertextual depen- dence1 are important for deciding the readability level. From this point of view, linguistic features ex- tracted from the whole passage actually provide us information from a holistic view, and it can be eas- ily integrated into the models we are using. In this paper, we integrate linguistic features with PLMs and conduct abundant experiments to analyze the effect of linguistic features on ARA from the per- spective of passage length. We find that:
• Evenwithsimplelinguisticfeatures,theaccuracy of PLMs on those small-scaled datasets (OneStop and RAZ) greatly improves by 9% and 22% re- spectively. Error analysis shows that all of the improvements are on long passages more than 510 tokens.
• The promotion of the features on PLMs becomes less significant when the dataset size exceeds ∼ 750 passages.
• Our results suggest that Newsela is possibly not suitable for ARA.
Also, we construct an up-to-date and high-quality dataset called RAZ from RAZ-Kid2’s printed lev- eled books. Though small-scaled, texts from this popular website make our research more practical. 
2 Data Analysis
To analyze the effect of linguistic features as precisely as we could, we select four different datasets namely Weebit (Vajjala and Meurers, 2012), Newsela (Xu et al., 2015), OneStopEnglish (Vajjala and Lucˇic ́, 2018) and RAZ. The character- istics of the 4 datasets are listed in Table 1.
Newsela is a text simplification dataset divided into 5 simplification levels. Texts from the hardest level are rephrased 4 times to create other 4 easier levels. Following previous works, we consider each simplification level a readability level.
Weebit is an ARA dataset. Texts from different readability levels focus on different topic. We sam- ple 625 instances each level to construct a balanced dataset.
OneStopEnglish is a relatively small text simpli- fication dataset containing 560 passages. Similar to Newsela, it is also constructed by rephrasing.
RAZ is an ARA dataset constructed by us. We select 370 passages from the RAZ-Kid2, an on- line education platform providing lots of leveled eBooks. We manually annotate them with 3 differ- ent readability levels according to the readability level criterion1. Compared to the above datasets, RAZ contains more text genres, topics and up-to- date vocabulary. More importantly, the average length of RAZ is much longer than the other three datasets, indicating that it is suitable for exploring the effect of linguistic features on long passages.
3 Method
Task Description Given a dataset D = {p1, p2, . . . , p|D|} with d readability levels C = {c1,c2,...,cd} . Each passage pi in dataset D is mapped to one label in C. It can be regarded as a classification task, a ranking task or an ordinal regression task. We take this task as a classification
task for its simplicity.
Model For each passage p = [xp1, xp2, .., xpL] which has L tokens, we concatenate our extracted linguistic features fp (see Table 4 for details) and the final hidden state of PLM hp to form vector Hp = [hp, fp]. We feed Hp into the classification head of PLM to get the predicted readability level of passage p. Depending on the range of the ex- tracted passage, there are two kinds of features fp: (1) ff ull is extracted from the whole passage, which provides a holistic view of the passage; (2) f
is extracted from the first 510 tokens of p when its length L is greater than 510, which provides the corresponding part of features w.r.t. the segment fed into the PLM. We also report the performance of statistic models using the same linguistic fea- tures for comparison.
the max sentence length as 512. We evaluate the model each 50 steps for 100 times. We use AdamW as our optimizer with the learning rate 1e-5 for the PLM encoder and learning rate 1e-3 for the PLM’s classification head. The size of train/dev/test set is listed in Tab 3. The linguistic features used in our work are listed in Table 4. We adopt the lexical and syntactic features from (Vajjala and Meurers, 2012) and add some common features from shallow, part- of-speech and discourse aspects. Please refer to our code for more details.
4 4.1
Results and Discussion
Effect of Linguistic Features: An Overview
In this section, we investigate how linguistic fea- tures affect PLMs’ performance on ARA. We as- sume that linguistic features promote PLM in two ways: First, they provide linguistic information that PLM is not good at capturing. Second, they provide information about the segment dropped by PLM, i.e. tokens longer than 510.
To verify our first assumption, we choose fpartial as fp to get H since fpartial are the exact corre- sponding part of features w.r.t. the segment fed into the PLM. Comparing the first and the second row of Table 2, we can see that PLM’s performance on RAZ and OneStop improves after adding the features. In Section 4.2, through error analysis, we find that the improvements are all on long pas- sages. The results on Weebit remain almost the same, there are two possible reasons: (1) (Lee et al., 2021) claim that "the max performance (91%) is already achieved on Weebit"; (2) Weebit is 5 to 8 times larger than RAZ and OneStop, such an amount of data is enough for the model to fit well. In Section 4.3, we further investigate the effect of features on different sizes of Weebit and find that features work when we decrease the size of Weebit. The results on Newsela are not as we expected, and we will discuss it in Section 4.4.
To verify our second assumption, we choose ff ull as fp to get H since ff ull provide information about the segment dropped by PLM. Adding these features further improves the PLM’s performance on RAZ and OneStop as expected. Specifically, the accuracy of PLMs on these small-scaled datasets greatly improves by 9% and 22% respectively.
4.2 Effect on Long and Short Passages
In order to further analyze on which passages do lin- guistic features promote PLM, we divide the whole dataset into long and short passages according to whether the passage exceeds 510 tokens. From Fig 2 (right) we can see that the PLM makes no mis- take on short passages of RAZ and OneStop. This indicates that the information captured by PLM is enough to classify the short passages even when the dataset is small. From Fig 2 (middle) we can see that fpartial reduce the mistakes on long passages without degrading the performance on short pas- sages, and ffull further improve the performance greatly, which supports our assumptions. The re- sults on Weebit and Newsela do not match our expections, but they do not conflict with our as- sumptions. We will discuss them in the following sections.
4.3 Analysis of Dataset Size
As discussed in Section 4.1, the features do not work on Weebit and Newsela. We guess it might be related to the size of dataset since Newsela and Weebit are much larger than RAZ and On- eStop (Fig. 1). To analyze the effect of dataset ize, we randomly sample 1%, 3%, 5%, 10%, 30%, 50%, 70% of the whole training set of Weebit and Newsela.
Fig. 3 shows that linguistic features signifi- cantly improve the PLM’s performance on long passages when the dataset size is small (less than 10%). However, as the size exceeds 30% (750 pas- sages)/10% (761 passages) for Weebit/Newsela , the promotion of the linguistic features on PLMs becomes less significant. Although the effect of linguistic features is less significant, we also find out that when the dataset size is between 10% and 50%, the results of PLM with features on both short passages and whole dataset are slightly better than PLM without features. This finding reveals that PLMs cannot learn how to deal with long passages without enough training data, and integrating lin- guistic features promotes PLMs on long passages. Different from what Lee et al. (2021) find, their simple PLM performs better than our model in the large dataset setting, this is because the features we use are relatively simple. Also, to analyze the effect of features, we do not ensemble traditional statistic models with PLMs, which further restricts the power of features. We think that simple fea- tures can already prove our assumptions, so we remain optimistic about the results when more so- phisticated features are used and better integration method is applied.
4.4 Text Simplification = ARA?
In this section, we claim that Newsela is possibly not suitable for ARA and consider it an explana- tion for why the results on Newsela do not meet our expectations. It should be pointed out that ARA fo- cuses on the absolute difficulty of a passage, while text simplification focuses on the relative ranking between different simplified versions of the origi- nal passage, which does not ensure one-to-one cor- respondence between the simplification level and readability level. Measuring the readability level by the Lexile grade just like prior work (Deutsch et al., 2020), we find there is overlap between classes. Specifically, Fig. 4 shows the confusion matrix be- tween the simplification level (SL) and the read- ability level (RL) on the train set. In order to study to what extent do the overlap affects the perfor- mance, we compare the test set accuracy between a non-overlapped set containing 118 passages and a same-sized overlapped set. The results averaged over three runs are 0.646 and 0.453. This indicates
that the overlap between classes does confuse the model. Although OneStop is also a text simpli- fication dataset, the three classes are designed to be strictly non-overlapping, thus making OneStop a clean dataset. The insignificant result also indi- cates that, while integrating linguistic features with PLMs in ARA is effective, it might not be effective for text simplification.
5 Conclusion
In this paper, we investigate how linguistic fea- tures promote PLMs on ARA from the perspective of passage length. Firstly, two self-proposed hy- potheses are proved: 1. Linguistic features provide linguistic information that PLM is not good at cap- turing; 2. Linguistic features provide information about the segment dropped by PLM. Secondly, we observe that the promotion of the features on PLMs becomes less significant when the dataset size ex- ceeds ∼750 passages. Thirdly, our results suggest that Newsela dataset is possibly not suitable for ARA.