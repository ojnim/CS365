Style transfer is the task of paraphrasing text
into a target-style domain while retaining the
content. Unsupervised approaches mainly focus on training a generator to rewrite input sentences. In this work, we assume that text styles
are determined by only a small proportion of
words; therefore, rewriting sentences via generative models may be unnecessary. As an alternative, we consider style transfer as a sequence
tagging task. Specifically, we use edit operations (i.e., deletion, insertion and substitution)
to tag words in an input sentence. We train a
classifier and a language model to score tagged
sequences and build a conditional random field.
Finally, the optimal path in the conditional random field is used as the output. The results
of experiments comparing models indicate that
our proposed model exceeds end-to-end baselines in terms of accuracy on both sentiment
and style transfer tasks with comparable or better content preservation.