Incorporating stronger syntactic biases into neural language models (LMs) is a long-standing
goal, but research in this area often focuses
on modeling English text, where constituent
treebanks are readily available. Extending
constituent tree-based LMs to the multilingual setting, where dependency treebanks are
more common, is possible via dependency-toconstituency conversion methods. However,
this raises the question of which tree formats
are best for learning the model, and for which
languages. We investigate this question by
training recurrent neural network grammars
(RNNGs) using various conversion methods,
and evaluating them empirically in a multilingual setting. We examine the effect on LM
performance across nine conversion methods
and five languages through seven types of syntactic tests. On average, the performance of
our best model represents a 19 % increase in
accuracy over the worst choice across all languages. Our best model shows the advantage
over sequential/overparameterized LMs, suggesting the positive effect of syntax injection in
a multilingual setting. Our experiments highlight the importance of choosing the right tree
formalism, and provide insights into making an
informed decision.