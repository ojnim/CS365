Recently neural network based approaches to
knowledge-intensive NLP tasks, such as question answering, started to rely heavily on the
combination of neural retrievers and readers.
Retrieval is typically performed over a large
textual knowledge base (KB) which requires
significant memory and compute resources, especially when scaled up. On HotpotQA we systematically investigate reducing the size of the
KB index by means of dimensionality (sparse
random projections, PCA, autoencoders) and
numerical precision reduction.
Our results show that PCA is an easy solution that requires very little data and is only
slightly worse than autoencoders, which are
less stable. All methods are sensitive to preand post-processing and data should always be
centered and normalized both before and after
dimension reduction. Finally, we show that it
is possible to combine PCA with using 1bit per
dimension. Overall we achieve (1) 100× compression with 75%, and (2) 24× compression
with 92% original retrieval performance.