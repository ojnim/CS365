{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CS365 Final Project\n",
    "\n",
    "\n",
    "Introduction and Conclusion part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random, string, re, spacy\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import transformers\n",
    "from transformers import pipeline, EncoderDecoderModel, AutoTokenizer\n",
    "from datasets import load_metric\n",
    "import sacrebleu\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn.functional as F\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaning\n",
    "def text_cleaner(text):\n",
    "    #remove words between ()\n",
    "    cleaned_string = \"\"\n",
    "    paren_depth = 0\n",
    "    for c in text:\n",
    "        if c == '(':\n",
    "            paren_depth += 1\n",
    "        elif c == ')' and paren_depth:\n",
    "            paren_depth -= 1\n",
    "        elif paren_depth == 0:\n",
    "            cleaned_string += c\n",
    "\n",
    "    newString = cleaned_string.lower()\n",
    "    newString = re.sub('\"','', newString)  \n",
    "    newString = re.sub(r\"'s\\b\",\"\",newString)\n",
    "    newString = re.sub(\"[^a-zA-Z]\", \" \", newString) \n",
    "\n",
    "    tokens = newString.split()\n",
    "    long_words = [i for i in tokens if len(i) > 1]\n",
    "\n",
    "    return (\" \".join(long_words)).strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training data\n",
    "paper_data_lstm = []\n",
    "abstract_data_lstm = []\n",
    "\n",
    "vocab_dict = {}\n",
    "\n",
    "#File\n",
    "paper_file_path = [\n",
    "    \"Comp_Ling_paper_{}.txt\".format(i) for i in range(1, 71)\n",
    "]\n",
    "abstract_file_path = [\n",
    "    \"abstract_{}.txt\".format(i) for i in range(1, 71)\n",
    "]\n",
    "\n",
    "for i in range(len(paper_file_path)):\n",
    "    with open(paper_file_path[i], \"r\") as f:\n",
    "        paper = f.read()\n",
    "\n",
    "    with open(abstract_file_path[i], \"r\") as f:\n",
    "        abstract = f.read()\n",
    "    \n",
    "    cleaned_paper = text_cleaner(paper)\n",
    "    cleaned_abstract = text_cleaner(abstract)\n",
    "\n",
    "    #Tokenization\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "    paper_tokens = [tok.text for tok in nlp(cleaned_paper)]\n",
    "    abstract_tokens = ['<sos>'] + [tok.text for tok in nlp(cleaned_abstract)] + ['<eos>']\n",
    "\n",
    "    for token in paper_tokens:\n",
    "        if token not in vocab_dict:\n",
    "            vocab_dict[token] = len(vocab_dict)\n",
    "    for token in abstract_tokens:\n",
    "        if token not in vocab_dict:\n",
    "            vocab_dict[token] = len(vocab_dict)\n",
    "            \n",
    "    paper_data_lstm.append(paper_tokens)\n",
    "    abstract_data_lstm.append(abstract_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test data\n",
    "test_paper = 'paper_ex.txt'\n",
    "#test_paper = \"Comp_Ling_paper_2.txt\"\n",
    "with open(test_paper, \"r\") as f:\n",
    "    test_text = f.read()\n",
    "\n",
    "cleaned_example = text_cleaner(test_text)\n",
    "test_tokens_original = word_tokenize(cleaned_example)\n",
    "\n",
    "for token in test_tokens_original:\n",
    "        if token not in vocab_dict:\n",
    "            vocab_dict[token] = len(vocab_dict)\n",
    "test_tokens = test_tokens_original[:2132]\n",
    "test_vocab = {token: idx for idx, token in enumerate(set(test_tokens))}\n",
    "test_vocab_size  = len(test_vocab)\n",
    "vocab_dict.update(test_vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = []\n",
    "paper_input = []\n",
    "abstract_input = []\n",
    "for i in range(len(paper_data_lstm)):\n",
    "    paper_vocab = {token: idx for idx, token in enumerate(set(paper_data_lstm[i]))}\n",
    "    abstract_vocab = {token: idx for idx, token in enumerate(set(abstract_data_lstm[i]))}\n",
    "\n",
    "    vocab_size.append([len(paper_vocab), len(abstract_vocab)])\n",
    "\n",
    "    paper_indices = [paper_vocab[token] for token in paper_data_lstm[i]]\n",
    "    abstract_indices = [abstract_vocab[token] for token in abstract_data_lstm[i]]\n",
    "\n",
    "    paper_indices_tensor = torch.tensor(paper_indices)\n",
    "    abstract_indices_tensor = torch.tensor(abstract_indices)\n",
    "\n",
    "    paper_input.append(paper_indices_tensor)\n",
    "    abstract_input.append(abstract_indices_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LSTM Model\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, paper_data, abstract_data):\n",
    "        self.paper_data = paper_data\n",
    "        self.abstract_data = abstract_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paper_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.paper_data[idx], self.abstract_data[idx]\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout=dropout, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src):\n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        outputs, (hidden, cell) = self.rnn(embedded)\n",
    "        return hidden, cell\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout=dropout, batch_first=True)\n",
    "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, input, hidden, cell):\n",
    "        input = input.unsqueeze(1)\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
    "        prediction = self.fc_out(output.squeeze(1))\n",
    "        return prediction, hidden, cell\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        \n",
    "    def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n",
    "        batch_size = trg.shape[0]\n",
    "        trg_len = trg.shape[1]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        \n",
    "        # tensor to store decoder outputs\n",
    "        outputs = torch.zeros(batch_size, trg_len, trg_vocab_size).to(self.device)\n",
    "        \n",
    "        # last hidden state of the encoder is used as the initial hidden state of the decoder\n",
    "        hidden, cell = self.encoder(src)\n",
    "        \n",
    "        # first input to the decoder is the <sos> tokens\n",
    "        input = trg[:,0]\n",
    "        \n",
    "        for t in range(1, trg_len):\n",
    "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
    "            outputs[:,t,:] = output\n",
    "            # decide if we will use teacher forcing or not\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            # get the highest predicted token from our predictions\n",
    "            top1 = output.argmax(1) \n",
    "            # if teacher forcing, use actual next token as next input. If not, use predicted token\n",
    "            input = trg[:,t] if teacher_force else top1\n",
    "        \n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, val_loader, criterion, device):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        for paper_data, abstract_data in val_loader:\n",
    "            paper_data, abstract_data = paper_data.to(device), abstract_data.to(device)\n",
    "            output = model(paper_data, abstract_data)\n",
    "            output_dim = output.shape[-1]\n",
    "            output = output[1:].reshape(-1, output_dim)\n",
    "            target = abstract_data[1:].contiguous().view(-1)\n",
    "            loss = criterion(output, target)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Train Loss: 6.23647928237915, Val Loss: 5.957086563110352\n",
      "Epoch [2/20], Train Loss: 5.827421188354492, Val Loss: 5.353848934173584\n",
      "Epoch [3/20], Train Loss: 4.931849002838135, Val Loss: 4.782479286193848\n",
      "Epoch [4/20], Train Loss: 4.070378303527832, Val Loss: 5.311925411224365\n",
      "Epoch [5/20], Train Loss: 4.331934452056885, Val Loss: 4.750409126281738\n",
      "Epoch [6/20], Train Loss: 4.0025458335876465, Val Loss: 4.276785373687744\n",
      "Epoch [7/20], Train Loss: 3.6773667335510254, Val Loss: 4.165059566497803\n",
      "Epoch [8/20], Train Loss: 3.6823103427886963, Val Loss: 4.106593132019043\n",
      "Epoch [9/20], Train Loss: 3.6944262981414795, Val Loss: 4.049398422241211\n",
      "Epoch [10/20], Train Loss: 3.614169120788574, Val Loss: 3.969383955001831\n",
      "Epoch [11/20], Train Loss: 3.5549328327178955, Val Loss: 3.915844440460205\n",
      "Epoch [12/20], Train Loss: 3.4680659770965576, Val Loss: 3.8978474140167236\n",
      "Epoch [13/20], Train Loss: 3.41715669631958, Val Loss: 3.914144515991211\n",
      "Epoch [14/20], Train Loss: 3.3750383853912354, Val Loss: 3.896158218383789\n",
      "Epoch [15/20], Train Loss: 3.390662670135498, Val Loss: 3.91430401802063\n",
      "Epoch [16/20], Train Loss: 3.292172431945801, Val Loss: 3.8990025520324707\n",
      "Epoch [17/20], Train Loss: 3.331415891647339, Val Loss: 3.8288629055023193\n",
      "Epoch [18/20], Train Loss: 3.267110824584961, Val Loss: 3.790599822998047\n",
      "Epoch [19/20], Train Loss: 3.264112949371338, Val Loss: 3.761245012283325\n",
      "Epoch [20/20], Train Loss: 3.179335832595825, Val Loss: 3.697213649749756\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_epochs  = 20\n",
    "batch_size = 512\n",
    "\n",
    "def my_collate_fn(batch):\n",
    "    paper_batch, abstract_batch = zip(*batch)\n",
    "    paper_padded = pad_sequence(paper_batch, batch_first=True, padding_value=0)\n",
    "    abstract_padded = pad_sequence(abstract_batch, batch_first=True, padding_value=0)\n",
    "    return paper_padded, abstract_padded\n",
    "\n",
    "paper_train, paper_val, abstract_train, abstract_val = train_test_split(\n",
    "    paper_input, abstract_input, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "train_dataset = MyDataset(paper_train, abstract_train)\n",
    "val_dataset = MyDataset(paper_val, abstract_val)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=my_collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=my_collate_fn)\n",
    "\n",
    "INPUT_DIM = 3000\n",
    "OUTPUT_DIM = 500\n",
    "ENC_EMB_DIM = 256\n",
    "DEC_EMB_DIM = 256\n",
    "HID_DIM = 512\n",
    "N_LAYERS = 2\n",
    "ENC_DROPOUT = 0.1\n",
    "DEC_DROPOUT = 0.1\n",
    "\n",
    "encoder = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)\n",
    "decoder = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT)\n",
    "\n",
    "model = Seq2Seq(encoder, decoder, device).to(device)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "#Training\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch_idx, (paper_data, abstract_data) in enumerate(train_loader):\n",
    "        paper_data, abstract_data = paper_data.to(device), abstract_data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(paper_data, abstract_data)\n",
    "        output_dim = output.shape[-1]\n",
    "        \n",
    "        output = output[1:].reshape(-1, output_dim)\n",
    "        target = abstract_data[1:].contiguous().view(-1)\n",
    "        \n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "    avg_val_loss = validate(model, val_loader, criterion, device)\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss}, Val Loss: {avg_val_loss}')\n",
    "\n",
    "torch.save(model.state_dict(), 'lstm_summarization_model_70_512.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Summary:\n",
      "generative concise work pretrained related performance generated such responses attribution generative grounded there motivating issues section dialogue neural marked motivating classification there natural designers sources text work user negation phenomena variety responses seek negation pretrained automatic classification framework large generates performance seek known he underlying pro phenomena documents accurate closely related basis forward was attribution advanced user signaling album accurate problem large addition sources hallucinate often if interdisciplinary experimental viewpoint faithfulness generative contains system phenomena requires sources large large large large large posals posals pretrained phenomena sources user performance contextual contributing being of using reported generates evaluation for seek classification faithfulness large underlying also according using closely accurate viewpoint faithfulness system \n"
     ]
    }
   ],
   "source": [
    "def generate_summary(model, tokens, paper_vocab, device):\n",
    "    text_indices = [paper_vocab[token] for token in tokens]\n",
    "    paper_data_tensor = torch.tensor(text_indices).unsqueeze(0).to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        #seq2seq\n",
    "        hidden, cell = model.encoder(paper_data_tensor)\n",
    "        trg = torch.tensor([paper_vocab['<sos>']]).to(device)\n",
    "\n",
    "        generated_tokens = []\n",
    "\n",
    "\n",
    "        for _ in range(198): #max summary length\n",
    "            output, hidden, cell = model.decoder(trg, hidden, cell)\n",
    "            output_probs = F.softmax(output, dim=1)\n",
    "            predicted_token = torch.multinomial(output_probs, 1).item()\n",
    "            generated_tokens.append(predicted_token)\n",
    "\n",
    "            if predicted_token == paper_vocab['<eos>'] or len(generated_tokens) >= 111:\n",
    "                break\n",
    "\n",
    "            trg = torch.tensor([predicted_token]).to(device)\n",
    "\n",
    "    return generated_tokens\n",
    "\n",
    "model = Seq2Seq(encoder, decoder, device)\n",
    "model.load_state_dict(torch.load('lstm_summarization_model_70_512.pth', map_location=device))\n",
    "model.eval()\n",
    "\n",
    "summary_tokens = generate_summary(model, test_tokens, vocab_dict, device)\n",
    "\n",
    "def get_key_from_value(dictionary, target_value):\n",
    "    for key, value in dictionary.items():\n",
    "        if value == target_value:\n",
    "            return key\n",
    "    return ' '\n",
    "\n",
    "summary_text = ''\n",
    "for token in summary_tokens:\n",
    "    word  = get_key_from_value(vocab_dict, token)\n",
    "    summary_text += word\n",
    "    summary_text += ' '\n",
    "\n",
    "print(\"Generated Summary:\")\n",
    "print(summary_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data_36 \n",
    "\n",
    "music coherence music of models old annotation he showing issues faithfulness which support translation related pretrained of closely george corroboration offers cues generative closely harrison is support relations interrelationship computational related old been large tributes large discuss advanced is summarization for evaluation coherence sources attribution support support broaden evaluation large"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/seungmincho/anaconda3/lib/python3.11/site-packages/datasets/load.py:756: FutureWarning: The repository for rouge contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/rouge/rouge.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Summary\n",
      "rouge1: 0.2703 (Precision), 0.0313 (Recall), 0.0562 (F1 Score)\n",
      "rouge2: 0.0000 (Precision), 0.0000 (Recall), 0.0000 (F1 Score)\n",
      "rougeL: 0.1171 (Precision), 0.0136 (Recall), 0.0243 (F1 Score)\n",
      "rougeLsum: 0.1171 (Precision), 0.0136 (Recall), 0.0243 (F1 Score)\n",
      "GPT Summary\n",
      "rouge1: 0.7879 (Precision), 0.1630 (Recall), 0.2701 (F1 Score)\n",
      "rouge2: 0.2995 (Precision), 0.0617 (Recall), 0.1023 (F1 Score)\n",
      "rougeL: 0.4697 (Precision), 0.0972 (Recall), 0.1610 (F1 Score)\n",
      "rougeLsum: 0.4697 (Precision), 0.0972 (Recall), 0.1610 (F1 Score)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/seungmincho/anaconda3/lib/python3.11/site-packages/datasets/load.py:756: FutureWarning: The repository for rouge contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/rouge/rouge.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def calculate_rouge_scores(predictions, references):\n",
    "    rouge = load_metric(\"rouge\")\n",
    "    results = rouge.compute(predictions=predictions, references=references)\n",
    "    return results\n",
    "\n",
    "#test data\n",
    "test_abstract = 'abstract_ex.txt'\n",
    "\n",
    "with open(test_paper, \"r\") as f:\n",
    "    test_abs = f.read()\n",
    "\n",
    "cleaned_example_abstract = text_cleaner(test_abs)\n",
    "\n",
    "# example summary and reference summary\n",
    "generated_summary = summary_text\n",
    "reference_summary = cleaned_example_abstract\n",
    "GPT_summary = 'The text discusses discourse relations (DRs) and their significance in understanding discourse coherence. It highlights the interdisciplinary nature of DRs, with linguistics focusing on their marking in discourse through corpus studies, while Natural Language Processing (NLP) aims at predicting discourse markers for various tasks. Despite these advancements, there is limited interdisciplinary dialogue. The study aims to address this gap by exploring the contribution of lexical semantics, specifically synonymy and antonymy, in signaling contrast and concession relations, and their interaction with different parts of speech (POS). The methodological approach involves computational modeling to analyze semantic signals in discourse relations, allowing for a transparent interpretation of results without manual coding. The study finds that adjectives, verbs, and nouns play a significant role in contrast relations compared to concession relations. Additionally, it observes differences between implicit and explicit relations. However, the approach has limitations, such as the need to consider phrasal verbs and sentence polarity in future research. Overall, the study contributes to bridging the gap between linguistic and computational approaches to discourse analysis. It offers insights into the role of lexical cues in discourse relations and proposes a method that can be automated for broader application in corpus linguistics research.'\n",
    "\n",
    "# Calculate ROUGE scores\n",
    "rouge_scores = calculate_rouge_scores([generated_summary], [reference_summary])\n",
    "rouge_scores_gpt = calculate_rouge_scores([GPT_summary], [reference_summary])\n",
    "\n",
    "# Print ROUGE scores\n",
    "print(\"Generated Summary\")\n",
    "for key, value in rouge_scores.items():\n",
    "    print(f\"{key}: {value.mid.precision:.4f} (Precision), {value.mid.recall:.4f} (Recall), {value.mid.fmeasure:.4f} (F1 Score)\")\n",
    "print(\"GPT Summary\")\n",
    "for key, value in rouge_scores_gpt.items():\n",
    "    print(f\"{key}: {value.mid.precision:.4f} (Precision), {value.mid.recall:.4f} (Recall), {value.mid.fmeasure:.4f} (F1 Score)\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
